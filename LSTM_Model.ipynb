{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "LSTM Model.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0-gImsV5zkl",
        "colab_type": "code",
        "colab": {},
        "outputId": "0cfe5636-fb61-4caa-8313-5b8b86da3a00"
      },
      "source": [
        "# create the word2vec dict from the dictionary\n",
        "\n",
        "import random as rand\n",
        "import pandas as pd\n",
        "from numpy import array\n",
        "from numpy import cumsum\n",
        "import scipy\n",
        "import tensorflow\n",
        "from keras import Sequential\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "import gensim\n",
        "\n",
        "embedding_path = \"wikipedia-pubmed-and-PMC-w2v.bin\"\n",
        "#embedding_path = \"glove.6B.100d.txt\"\n",
        "#embedding_path = \"glove.twitter.27B.100d.txt\" ## change \n",
        "\n",
        "def get_word2vec(file_path):\n",
        "    file = open(embedding_path, \"r\", encoding=\"utf-8\")\n",
        "    if (file):\n",
        "        word2vec = dict()\n",
        "        split = file.read().splitlines()\n",
        "        for line in split:\n",
        "            key = line.split(' ',1)[0] # the first word is the key\n",
        "            value = np.array([float(val) for val in line.split(' ')[1:]])\n",
        "            word2vec[key] = value\n",
        "        return (word2vec)\n",
        "    else:\n",
        "        print(\"invalid file path\")\n",
        "        \n",
        "def load_bin_vec(fname): \n",
        "    \"\"\" \n",
        "    Loads 300x1 word vecs from Google (Mikolov) word2vec \n",
        "    \"\"\" \n",
        "    word_vecs = {} \n",
        "    with open(fname, \"rb\") as f: \n",
        "        header = f.readline() \n",
        "        vocab_size, layer1_size = map(int, header.split()) \n",
        "        binary_len = np.dtype('float32').itemsize * layer1_size \n",
        "        for line in range(vocab_size): \n",
        "            word = [] \n",
        "            while True: \n",
        "                ch = f.read(1) \n",
        "                if ch == ' ': \n",
        "                    word = ''.join(word) \n",
        "                    break \n",
        "                if ch != '\\n': \n",
        "                    word.append(ch)   \n",
        "            word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')   \n",
        "    return word_vecs\n",
        "\n",
        "#w2v = get_word2vec(embedding_path)\n",
        "#w2v = load_bin_vec(embedding_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MemoryError",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-3-e03e8c2333c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m#w2v = get_word2vec(embedding_path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[0mw2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_bin_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-3-e03e8c2333c1>\u001b[0m in \u001b[0;36mload_bin_vec\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m     45\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                     \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m             \u001b[0mword_vecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mword_vecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mMemoryError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKOFNZa_5zkq",
        "colab_type": "code",
        "colab": {},
        "outputId": "acd77bc3-aae4-42c3-abe6-54a78fafd6e8"
      },
      "source": [
        "import gensim\n",
        "#model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-7-e875d99f9e35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_TkFbtH5zks",
        "colab_type": "code",
        "colab": {},
        "outputId": "c0e61561-dde7-44c5-c156-c9e0886aaa7a"
      },
      "source": [
        "# Test harness for word2vec function\n",
        "\n",
        "import random\n",
        "\n",
        "for i in range(10):\n",
        "    sample = random.choice(list(w2v.keys()))\n",
        "    print(len(w2v[sample]))\n",
        "    print(w2v[sample])\n",
        "    print(sample)\n",
        "    \n",
        "print(len(w2v.keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'w2v' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-8-eefdee46285e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'w2v' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvOYNk5K5zkw",
        "colab_type": "code",
        "colab": {},
        "outputId": "9b393677-df94-45f3-9d14-872910d5d5d6"
      },
      "source": [
        "# Loading in our indication data; returns a dataframe and a dictionary[sentence] = label\n",
        "\n",
        "def load_FDA_data():\n",
        "    file = open(\"paranoia4_no_commas.csv\", 'r')\n",
        "    data_dict = {}\n",
        "    df = pd.DataFrame()\n",
        "    for line in file:\n",
        "        current_line = line.split(',')[-2:]\n",
        "        if len(current_line) == 2:\n",
        "            key = current_line[0]\n",
        "            item = current_line[1].strip()\n",
        "            data_dict[key] = item\n",
        "            temp_series = pd.Series([current_line[0], current_line[1].strip()])\n",
        "            df = df.append(temp_series, ignore_index=True)\n",
        "    return df, data_dict\n",
        "\n",
        "df, data_dict = load_FDA_data()\n",
        "print(df)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                      0  1\n",
            "0      minocycline hydrochloride tablets are indicat...  1\n",
            "1     respiratory tract infections caused by mycopla...  1\n",
            "2     lymphogranuloma venereum caused by chlamydia t...  1\n",
            "3     psittacosis ( ornithosis ) due to chlamydia ps...  1\n",
            "4     trachoma caused by chlamydia trachomatis  alth...  1\n",
            "5     inclusion conjunctivitis caused by chlamydia t...  1\n",
            "6     nongonococcal urethritis  endocervical  or rec...  1\n",
            "7           relapsing fever due to borrelia recurrentis  1\n",
            "8     chancroid caused by haemophilus ducreyi plague...  1\n",
            "9               tularemia due to francisella tularensis  1\n",
            "10                    cholera caused by vibrio cholerae  1\n",
            "11    campylobacter fetus infections caused by campy...  1\n",
            "12    brucellosis due to brucella species ( in conju...  1\n",
            "13        bartonellosis due to bartonella bacilliformis  1\n",
            "14    granuloma inguinale caused by calymmatobacteri...  1\n",
            "15    minocycline is indicated for treatment of infe...  1\n",
            "16    respiratory tract infections caused by haemoph...  1\n",
            "17                                    use as prescribed  0\n",
            "18     libido formulated for symptoms associated wit...  1\n",
            "19     hyperactivity mental focus formulated for sym...  1\n",
            "20     incontinence formulated for symptoms associat...  1\n",
            "21     uses for temporary relief of confusion about ...  1\n",
            "22    product name indications section prostate form...  1\n",
            "23    indications for temporary relief of hunger sen...  1\n",
            "24    indications for temporary relief of acute stre...  1\n",
            "25     oral care gingivitis formulated for symptoms ...  1\n",
            "26     injury rescue formulated for symptoms associa...  1\n",
            "27     fear phobia formulated for symptoms associate...  1\n",
            "28     warts moles skin tags formulated for various ...  1\n",
            "29    uses temporarily relieves nausea  vomiting  pa...  1\n",
            "...                                                 ... ..\n",
            "7834  1  tetrabenazine tablets are indicated for the...  1\n",
            "7835  tetrabenazine is a vesicular monoamine transpo...  1\n",
            "7836   bupropion hydrochloride extended release tabl...  1\n",
            "7837  ( 1 ) 1 1 major depressive disorder bupropion ...  1\n",
            "7838  1 2 seasonal affective disorder bupropion hydr...  1\n",
            "7839  1  ravicti is indicated for use as a nitrogen ...  1\n",
            "7840  ravicti must be used with dietary protein rest...  4\n",
            "7841  limitations of use ravicti is not indicated fo...  2\n",
            "7842  the safety and efficacy of ravicti for the tre...  0\n",
            "7843  ravicti is a nitrogen binding agent indicated ...  1\n",
            "7844  ravicti must be used with dietary protein rest...  4\n",
            "7845  ( 1 ) limitations of use ravicti is not indica...  0\n",
            "7846  ( 1 ) safety and efficacy for treatment of n a...  0\n",
            "7847  1  lucentis is indicated for the treatment of ...  1\n",
            "7848   metopirone is a diagnostic drug for testing h...  1\n",
            "7849   l cysteine hydrochloride injection  usp is in...  1\n",
            "7850   nalbuphine hydrochloride injection is indicat...  1\n",
            "7851  nalbuphine hydrochloride injection can also be...  1\n",
            "7852  limitations of use because of the risks of add...  4\n",
            "7853  1  aptivus  co administered with ritonavir  is...  1\n",
            "7854  this indication is based on analyses of plasma...  0\n",
            "7855  the adult studies were conducted in clinically...  0\n",
            "7856  the following points should be considered when...  0\n",
            "7857  the use of other active agents with aptivus ri...  0\n",
            "7858  genotypic or phenotypic testing and or treatme...  0\n",
            "7859  the number of baseline primary protease inhibi...  0\n",
            "7860  use caution when prescribing aptivus ritonavir...  2\n",
            "7861  liver function tests should be performed at in...  4\n",
            "7862  the drug drug interaction potential of aptivus...  2\n",
            "7863  use caution when prescribing aptivus ritonavir...  3\n",
            "\n",
            "[7864 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jvIkmif5zky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizing (breaking sentences into words. \n",
        "# Gets tricky because there are a few different Tokenizer vs Tokenize classes from different packages.\n",
        "\n",
        "import nltk\n",
        "# nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = set(stopwords.words('english'))\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('stopwords')\n",
        "from nltk.tokenize import *\n",
        "from nltk.corpus import wordnet as wn\n",
        "tknzr = TweetTokenizer()\n",
        "\n",
        "\n",
        "def get_tokens(sentence):\n",
        "#     tokens = nltk.word_tokenize(sentence)  # now using tweet tokenizer\n",
        "    tokens = tknzr.tokenize(sentence)\n",
        "    tokens = [token for token in tokens if (token not in stopwords and len(token) > 1)]\n",
        "    tokens = [get_lemma(token) for token in tokens]\n",
        "    return (tokens)\n",
        "\n",
        "\n",
        "def get_lemma(word):\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "        return lemma\n",
        "    \n",
        "    \n",
        "token_list = (df[0].apply(get_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4Q0InD75zk0",
        "colab_type": "code",
        "colab": {},
        "outputId": "711db790-e27d-4e2e-dcc0-740a57383c07"
      },
      "source": [
        "# Quick test harness for tokenization functions.\n",
        "\n",
        "#print(type(token_list))\n",
        "#print(\"\")\n",
        "#print(token_list)\n",
        "#print(\"\")\n",
        "#print(tknzr)\n",
        "\n",
        "l = token_list.tolist()\n",
        "\n",
        "flat_list = []\n",
        "for sublist in l:\n",
        "    for item in sublist:\n",
        "        flat_list.append(item)\n",
        "        \n",
        "pd.Series(flat_list).value_counts()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "indicate         3339\n",
              "treatment        3001\n",
              "patient          2852\n",
              "use              1917\n",
              "therapy           980\n",
              "adult             972\n",
              "infection         843\n",
              "disease           765\n",
              "tablet            734\n",
              "see               698\n",
              "drug              676\n",
              "active            655\n",
              "clinical          611\n",
              "study             585\n",
              "combination       581\n",
              "include           530\n",
              "cause             524\n",
              "symptom           519\n",
              "acute             512\n",
              "injection         492\n",
              "associate         480\n",
              "ingredient        471\n",
              "years             467\n",
              "may               467\n",
              "age               465\n",
              "risk              464\n",
              "chronic           458\n",
              "unii              457\n",
              "usp               442\n",
              "limitation        435\n",
              "                 ... \n",
              "nebulization        1\n",
              "generator           1\n",
              "dizzy               1\n",
              "anuric              1\n",
              "expander            1\n",
              "ahealon             1\n",
              "6940                1\n",
              "student             1\n",
              "bloom               1\n",
              "carmine             1\n",
              "infract             1\n",
              "living              1\n",
              "protector           1\n",
              "safely              1\n",
              "viper               1\n",
              "periodontitis       1\n",
              "cocs                1\n",
              "sheer               1\n",
              "icodextrin          1\n",
              "pyridone            1\n",
              "expectedthe         1\n",
              "iodobenzoic         1\n",
              "lengthening         1\n",
              "cyanosis            1\n",
              "grief               1\n",
              "bal                 1\n",
              "optimally           1\n",
              "flight              1\n",
              "5052                1\n",
              "protirelin          1\n",
              "Length: 9730, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2T-yI135zk3",
        "colab_type": "code",
        "colab": {},
        "outputId": "65a8fc1f-8578-4f6e-c83c-c2f34e64606c"
      },
      "source": [
        "# Converting tokens to integers. Beware Tokenizer from keras (vs tokenize from nltk).\n",
        "# If there are bug(s), I'd expect one to be here. This might all be redundant too, as another Tokenizer is used in the model.\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import keras\n",
        "tokenizer = Tokenizer(num_words = 850)  # change num_words as needed\n",
        "tokenizer.fit_on_texts(df[0].tolist())  # no idea what this line is doing. maybe building a vocabulary?\n",
        "sequences = tokenizer.texts_to_sequences(df[0].tolist())\n",
        "\n",
        "encoded_docs = sequences  \n",
        "# pad documents to a max length of 4 words\n",
        "X = keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\n",
        "\n",
        "print(X.shape)\n",
        "_, max_len = X.shape\n",
        "print(max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7864, 189)\n",
            "189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eoQblMc5zk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test Harness for token--> integer conversion\n",
        "\n",
        "\n",
        "#from random import *\n",
        "\n",
        "#print(len(sequences))\n",
        "\n",
        "#for i in range(10):\n",
        " #   print(sequences[rand.randint(1, len(sequences)-1)])\n",
        "#print(len(X))\n",
        "#for i in range(100):\n",
        " #   print(len(X[rand.randint(1, len(X)-1)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GscHDKSM5zk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splits into training and testing data. Also reduces the dataset to a binary. Worth noting that all sentences marked \"unsure\"\n",
        "# were here marked as not indications, so only clear indications receive a 1 and all else is 0.\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import *\n",
        "from sklearn.svm import *\n",
        "from sklearn.metrics import *\n",
        "\n",
        "#le = preprocessing.LabelEncoder()\n",
        "#le.classes_ = 3\n",
        "#Y_new = le.fit_transform(df[1])\n",
        "Y_new = array(df[1].replace({'u': '0', 'c': '0', '2': '0', '3': '0', '4': '0', '':'0', '\"Active Ingredient UNII(s)':'0', '/':'0'}))  # should not need to do this once you have polished data\n",
        "Y_new = Y_new.astype(int)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y_new, test_size=0.20, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Syo9L0pF5zk-",
        "colab_type": "code",
        "colab": {},
        "outputId": "17d38d9f-d8d3-406e-dcc2-602fb1da1c65"
      },
      "source": [
        "# Testing if the shuffling is actually working.\n",
        "# Spoiler: it is.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "a1 = array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "a2 = array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "\n",
        "X_tr_fake, X_te_fake, Y_tr_fake, Y_te_fake = train_test_split(a1, a2, test_size=0.2, shuffle=True)\n",
        "print(X_tr_fake, X_te_fake, \"\\n\", Y_tr_fake, Y_te_fake)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 4  1  7  8 10  2  5  3] [9 6] \n",
            " [ 4  1  7  8 10  2  5  3] [9 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxz1Q1Yp5zlA",
        "colab_type": "code",
        "colab": {},
        "outputId": "b790f4ba-ffd8-4322-85a3-78f7006c8bfd"
      },
      "source": [
        "# Another test harness to make sure the above is actually splitting data, and to make sure shapes of arrays feed into\n",
        "# the model properly.\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(df[1].max())\n",
        "print(X_train)\n",
        "print(\"Y_new:\")\n",
        "print(Y_new)\n",
        "Y_new.min()\n",
        "pd.Series(Y_new).value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6291, 189)\n",
            "(1573, 189)\n",
            "u\n",
            "[[166  74 292 ...   0   0   0]\n",
            " [308  15  19 ...   0   0   0]\n",
            " [537   3 658 ...   0   0   0]\n",
            " ...\n",
            " [534 320   4 ...   0   0   0]\n",
            " [  5  78 192 ...   0   0   0]\n",
            " [ 32  48  50 ...   0   0   0]]\n",
            "Y_new:\n",
            "[1 1 1 ... 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    4109\n",
              "0    3755\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-0-GOtw5zlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This runs an SVM on the data. Takes a while to run.\n",
        "\n",
        "\n",
        "def svm_wrapper(X_train,Y_train):\n",
        "    param_grid = [\n",
        "    {'C': [1, 10], 'kernel': ['linear']},\n",
        "    {'C': [1, 10], 'gamma': [0.1,0.01], 'kernel': ['rbf']},]\n",
        "    svm = GridSearchCV(SVC(),param_grid)\n",
        "    svm.fit(X_train, Y_train)\n",
        "    return(svm)\n",
        "\n",
        "#svm = svm_wrapper(X_train,Y_train)\n",
        "#Y_pred = svm.predict(X_test)\n",
        "#score = accuracy_score(Y_test,Y_pred)\n",
        "#print(\"accuracy :\", score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP8Pryd45zlE",
        "colab_type": "code",
        "colab": {},
        "outputId": "af5931ff-0709-4922-fc1b-a3f04b8d5350"
      },
      "source": [
        "# This is the main function itself. \n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from numpy import zeros\n",
        "\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(token_list)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "### Below block is commented out as I think this was already done.\n",
        "\n",
        "# integer encode the documents\n",
        "# encoded_docs = t.texts_to_sequences(token_list)\n",
        "# pad documents to a max length of 4 words\n",
        "# max_length = max_len = 10\n",
        "# X = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# y = Y_new\n",
        "\n",
        "embedding_matrix = zeros((vocab_size, 100))  # 100 because the original data is a 100 vector\n",
        "\n",
        "#  This is creating an embedding matrix, but not quite sure what or why.\n",
        "for word, i in t.word_index.items():\n",
        "    embedding_vector = w2v.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "        \n",
        "### Main Model Building\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "input = Input(shape=(max_len,))\n",
        "model = Embedding(vocab_size,100,weights=[embedding_matrix],input_length=max_len)(input)\n",
        "model = Bidirectional(LSTM(100,return_sequences=True,dropout=0.50),merge_mode='concat')(model)\n",
        "model = TimeDistributed(Dense(100,activation='relu'))(model)\n",
        "model = Flatten()(model)\n",
        "model = Dense(100,activation='relu', kernel_regularizer=regularizers.l2(.01))(model)\n",
        "output = Dense(1,activation='sigmoid')(model)\n",
        "model = Model(input,output)\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
        "# give a loss as the model is working\n",
        "model.fit(X_train, Y_train, verbose=1, epochs=3)\n",
        "#print('Accuracy: %f' % (accuracy*100))\n",
        "\n",
        "Y_pred = model.predict(X_test, verbose=1) # scrutinize!\n",
        "y_pred = np.array([np.argmax(pred) for pred in Y_pred])\n",
        "\n",
        "print('  Classification Report:\\n',classification_report(Y_test, Y_pred.round()),'\\n')\n",
        "print('Confusion matrix:', confusion_matrix(Y_test, Y_pred.round()))\n",
        "\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#print(\" You can safely ignore the \\'2' column zeroes as they just say there was no data for that class. I ran a category classifier on a binary problem.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\abhatt\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From C:\\Users\\abhatt\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From C:\\Users\\abhatt\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/3\n",
            "6291/6291 [==============================] - 88s 14ms/step - loss: 0.5827 - acc: 0.7697\n",
            "Epoch 2/3\n",
            "6291/6291 [==============================] - 86s 14ms/step - loss: 0.3196 - acc: 0.8822\n",
            "Epoch 3/3\n",
            "6291/6291 [==============================] - 86s 14ms/step - loss: 0.2706 - acc: 0.9054\n",
            "1573/1573 [==============================] - 5s 3ms/step\n",
            "  Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.91      0.91       743\n",
            "           1       0.92      0.92      0.92       830\n",
            "\n",
            "   micro avg       0.92      0.92      0.92      1573\n",
            "   macro avg       0.92      0.92      0.92      1573\n",
            "weighted avg       0.92      0.92      0.92      1573\n",
            " \n",
            "\n",
            "Confusion matrix: [[677  66]\n",
            " [ 67 763]]\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 189)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 189, 100)          973100    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 189, 200)          160800    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 189, 100)          20100     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 18900)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               1890100   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 3,044,201\n",
            "Trainable params: 3,044,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGHbTiG05zlG",
        "colab_type": "code",
        "colab": {},
        "outputId": "fcfea373-6108-417a-99ea-38fa76f6e05e"
      },
      "source": [
        "print(X.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)\n",
        "pd.Series(Y_test).value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7864, 189)\n",
            "(6291,)\n",
            "(1573,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    830\n",
              "0    743\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfZOcJ8Q5zlL",
        "colab_type": "code",
        "colab": {},
        "outputId": "f7f6b88e-38de-4171-9b05-44f95e9479fb"
      },
      "source": [
        "#print('  Classification Report:\\n',classification_report(Y_test, y_pred),'\\n')\n",
        "#print('Confusion matrix:', confusion_matrix(Y_test, y_pred))\n",
        "\n",
        "print('  Classification Report:\\n',classification_report(Y_test, Y_pred.round()),'\\n')\n",
        "print('Confusion matrix:', confusion_matrix(Y_test, Y_pred.round()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.91      0.91       743\n",
            "           1       0.92      0.92      0.92       830\n",
            "\n",
            "   micro avg       0.92      0.92      0.92      1573\n",
            "   macro avg       0.92      0.92      0.92      1573\n",
            "weighted avg       0.92      0.92      0.92      1573\n",
            " \n",
            "\n",
            "Confusion matrix: [[677  66]\n",
            " [ 67 763]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
