{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "preprocessing_labeling_xml.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1BlAnTxg5nM",
        "colab_type": "code",
        "colab": {},
        "outputId": "a71311a2-cc7b-40f9-da89-a4f67429a0f2"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "from nltk.corpus import stopwords\n",
        "import warnings \n",
        "import gensim \n",
        "from gensim.models import Word2Vec \n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk import tokenize\n",
        "import pandas as pd\n",
        "import xlrd\n",
        "import os\n",
        "import textblob\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ZLiu\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ZLiu\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGv8aLFTg5ni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"C:/Users/ZLiu/Desktop/xiangwen/RX_label\")\n",
        "files = os.listdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PARb3nkHg5oK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "digits = \"([0-9])\"\n",
        "def noiseRemoval(st):\n",
        "    st = st.replace('\\n', ' ').replace('\\r', '')\n",
        "    st = st.replace(\"-\", \" \")\n",
        "    st = st.replace(\"â€™\", \" \")\n",
        "    st = st.replace(\"â\", \" \")\n",
        "    st = st.replace(\"â€™s\", \" \")y\n",
        "    st = re.sub(r\"[.,/?\\'\\\"\\\\;:\\[\\]{}!@#$^()]+\", \" \", st, re.DOTALL|re.U|re.I)\n",
        "    st = re.sub(r\" [a-z0-9]{1} \", \" \", st)\n",
        "    st = st.replace(\"&\",\"\")\n",
        "    return st"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdeKQIJSg5oN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_tokenizer(s):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = nltk.tokenize.word_tokenize(s) # split string into words (tokens)\n",
        "    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n",
        "    tokens = [t for t in tokens if t not in stop_words] # remove stopwords\n",
        "    tokens = [t for t in tokens if not any(c.isdigit() for c in t)] # remove any digits, i.e. \"3rd edition\"\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXsBTUhHg5oP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_preprocessing(s,filtered_sentence):\n",
        "    for words in s:\n",
        "        word_tokens = noiseRemoval(words)\n",
        "        word_tokens = my_tokenizer(word_tokens) \n",
        "        filtered_sentence.append(word_tokens)\n",
        "    return filtered_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrfLJGR5g5oR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered_sentence = []\n",
        "for f in files:\n",
        "    with open(f,encoding=\"utf8\") as xlmfile:\n",
        "        soup =  BeautifulSoup(xlmfile, 'lxml')\n",
        "    s = sent_tokenize(soup.text.lower())\n",
        "    filtered_sentence = sentence_preprocessing(s,filtered_sentence)\n",
        "    del s, soup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF_6UWYVg5oU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import pickle\n",
        "# pickle.dump(filtered_sentence,open(\"labeling_sentences.p\",\"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSh9m6oGg5oW",
        "colab_type": "code",
        "colab": {},
        "outputId": "b51b6cb4-fc4f-4433-8315-9b49f8fefc17"
      },
      "source": [
        "# Create CBOW model \n",
        "# model1 = gensim.models.Word2Vec(filtered_sentence, min_count = 1,  \n",
        "                              size = 10, window = 3) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
            "  \"C extension not loaded, training will be slow. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmQ8_FHxg5oZ",
        "colab_type": "code",
        "colab": {},
        "outputId": "cfc6a21a-8e91-4a91-d618-8ddf924ebca4"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "synonyms = []\n",
        "antonyms = []\n",
        "for syn in wordnet.synsets(\"steatosis\"):\n",
        "    for l in syn.lemmas():\n",
        "        synonyms.append(l.name())\n",
        "        if l.antonyms():\n",
        "            antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "print(set(synonyms))\n",
        "print(set(antonyms))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "set()\n",
            "set()\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
